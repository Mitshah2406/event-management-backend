

services:
  # Main Application
  app:
    build:
      context: ../../
      dockerfile: ./deployments/docker/Dockerfile.prod
    container_name: evently_app
    restart: unless-stopped
    ports:
      - "9000:8080"
    env_file:
      - ../../.env  # Let Docker Compose handle the .env file
    environment:
      # Server Configuration
      - PORT=8080
      - GIN_MODE=release
      - API_VERSION=v1
      - API_PREFIX=/api
      
      # Database Configuration
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-evently_prod}
      - DB_USER=${DB_USER:-evently_user}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_SSLMODE=disable
      
      # Redis Configuration
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - REDIS_DB=0
      
      # Kafka Configuration
      - KAFKA_BROKERS=kafka:9093
      - KAFKA_CLIENT_ID=evently-backend
      - KAFKA_AUTO_OFFSET_RESET=earliest
      - KAFKA_ENABLE_AUTO_COMMIT=true
      
      # JWT Configuration
      - JWT_SECRET=${JWT_SECRET}
      - JWT_EXPIRES_IN=900
      - JWT_REFRESH_EXPIRES_IN=86400
      
      # Rate Limiting
      - RATE_LIMIT_ENABLED=true
      - RATE_LIMIT_WINDOW_DURATION=60s
      - RATE_LIMIT_DEFAULT_REQUESTS=100
      - RATE_LIMIT_PUBLIC_REQUESTS=200
      - RATE_LIMIT_AUTH_REQUESTS=30
      - RATE_LIMIT_BOOKING_REQUESTS=50
      - RATE_LIMIT_ADMIN_REQUESTS=300
      
      # Email Configuration (optional for initial deployment)
      - SMTP_HOST=${SMTP_HOST:-}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USERNAME=${SMTP_USERNAME:-}
      - SMTP_PASSWORD=${SMTP_PASSWORD:-}
      - FROM_EMAIL=${FROM_EMAIL:-noreply@evently.com}
      
      # Logging
      - LOG_LEVEL=info
    
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - evently_network
    volumes:
      - app_uploads:/app/uploads
      - app_logs:/app/logs
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:${PORT}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: ${APP_MEMORY_LIMIT:-512m}
          cpus: '${APP_CPU_LIMIT:-1.0}'
        reservations:
          memory: 256m
          cpus: '0.5'

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: evently_postgres
    restart: unless-stopped
    ports:
      - "9001:${DB_PORT}"
    environment:
      - POSTGRES_DB=${DB_NAME}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=md5
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - evently_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    command: >
      postgres -c max_connections=${DB_MAX_OPEN_CONNS:-100}
               -c shared_buffers=128MB
               -c effective_cache_size=512MB
               -c maintenance_work_mem=32MB
               -c checkpoint_completion_target=0.9
               -c wal_buffers=8MB
               -c default_statistics_target=100
               -c work_mem=2MB
    deploy:
      resources:
        limits:
          memory: ${POSTGRES_MEMORY_LIMIT:-256m}
          cpus: '${POSTGRES_CPU_LIMIT:-0.5}'
        reservations:
          memory: 128m
          cpus: '0.25'

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: evently_redis
    restart: unless-stopped
    ports:
      - "9002:${REDIS_PORT}"
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - evently_network
    healthcheck:
      test: |
        if [ -n "${REDIS_PASSWORD}" ]; then
          redis-cli --pass "${REDIS_PASSWORD}" ping
        else
          redis-cli ping
        fi
      interval: 10s
      timeout: 3s
      retries: 5
    command: >
      sh -c "
      if [ -n '${REDIS_PASSWORD}' ]; then
        redis-server 
        --requirepass ${REDIS_PASSWORD}
        --appendonly yes 
        --maxmemory 256mb 
        --maxmemory-policy allkeys-lru
        --save 900 1
        --save 300 10
        --save 60 10000
      else
        redis-server 
        --appendonly yes 
        --maxmemory 256mb 
        --maxmemory-policy allkeys-lru
        --save 900 1
        --save 300 10
        --save 60 10000
      fi
      "
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_LIMIT:-128m}
          cpus: '${REDIS_CPU_LIMIT:-0.25}'
        reservations:
          memory: 64m
          cpus: '0.1'

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.4
    container_name: evently_zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - evently_network
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: ${ZOOKEEPER_MEMORY_LIMIT:-256m}
          cpus: '${ZOOKEEPER_CPU_LIMIT:-0.5}'
        reservations:
          memory: 128m
          cpus: '0.25'

  # Kafka Message Broker
  kafka:
    image: confluentinc/cp-kafka:7.4.4
    container_name: evently_kafka
    restart: unless-stopped
    ports:
      - "9003:9092"  # External access
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9003,PLAINTEXT_INTERNAL://kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_LOG_RETENTION_MS: 604800000  # 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - evently_network
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: ${KAFKA_MEMORY_LIMIT:-512m}
          cpus: '${KAFKA_CPU_LIMIT:-1.0}'
        reservations:
          memory: 256m
          cpus: '0.5'

volumes:
  postgres_data:
    driver: local
    name: evently_postgres_data
  redis_data:
    driver: local
    name: evently_redis_data
  app_uploads:
    driver: local
    name: evently_app_uploads
  app_logs:
    driver: local
    name: evently_app_logs
  zookeeper_data:
    driver: local
    name: evently_zookeeper_data
  zookeeper_logs:
    driver: local
    name: evently_zookeeper_logs
  kafka_data:
    driver: local
    name: evently_kafka_data

networks:
  evently_network:
    driver: bridge
    name: evently_network
